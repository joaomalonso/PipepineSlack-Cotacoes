from airflow import DAG
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator
from datetime import datetime

with DAG(
    'Executando-notebook-etl',
    start_date=datetime(2025, 9, 6),
    schedule_interval="0 9 * * *",  # Todos os dias Ã s 9h
    catchup=True
) as dag_executando_notebook_extracao:
    
    extraindo_dados = DatabricksRunNowOperator(
    task_id='Extraindo-conversoes',
    databricks_conn_id='databricks_default',
    job_id=79256***342450,
    json={
        "notebook_params": {
            "data_execucao": "{{ data_interval_end | ds }}"
        }
    }
)

transformando_dados = DatabricksRunNowOperator(
    task_id='transformando-dados',
    databricks_conn_id='databricks_default',
    job_id=10959***2895584,
)

enviando_relatorio = DatabricksRunNowOperator(
    task_id='enviando-relatorio',
    databricks_conn_id='databricks_default',
    job_id=64539***3789472,
)

extraindo_dados >> transformando_dados >> enviando_relatorio